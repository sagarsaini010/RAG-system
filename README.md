# ğŸš€ Gemini + Pinecone RAG System (Node.js)

![Node.js](https://img.shields.io/badge/Node.js-20+-green?logo=node.js)
![LangChain](https://img.shields.io/badge/LangChain-JS-blue)
![Google Gemini](https://img.shields.io/badge/Google-Gemini-orange?logo=google)
![Pinecone](https://img.shields.io/badge/VectorDB-Pinecone-purple)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)

A **production-ready Retrieval-Augmented Generation (RAG) system** built with:

- ğŸ§  Google Gemini (Embeddings + Chat Model)
- ğŸ“¦ Pinecone (Vector Database)
- ğŸ”— LangChain (JavaScript)
- âš¡ Node.js

---

## ğŸ“Œ Overview

This project demonstrates a complete RAG pipeline:

1. ğŸ“„ Load a PDF document  
2. âœ‚ï¸ Split into chunks  
3. ğŸ§® Convert chunks into embeddings  
4. ğŸ—„ Store embeddings in Pinecone  
5. â“ Accept user queries  
6. ğŸ” Retrieve relevant chunks  
7. ğŸ¤– Generate grounded answers using Gemini  

---

## ğŸ— Architecture

PDF
â†“
Chunking
â†“
Gemini Embeddings (3072)
â†“
Pinecone Vector Storage
â†“
User Query
â†“
Query Embedding
â†“
Similarity Search
â†“
Context + Question
â†“
Gemini Chat Model
â†“
Final Answer
```
---

## ğŸ›  Tech Stack

| Technology | Purpose |
|------------|----------|
| Node.js | Runtime |
| LangChain JS | Orchestration |
| Gemini | Embeddings + LLM |
| Pinecone | Vector Database |
| pdf-parse | PDF Processing |

---
```
## ğŸ“‚ Project Structure

```
RAG-system/
â”‚
â”œâ”€â”€ indexing.js # PDF â†’ Chunk â†’ Embed â†’ Pinecone
â”œâ”€â”€ query.js # Query â†’ Retrieve â†’ Generate Answer
â”œâ”€â”€ sample.pdf # Knowledge source
â”œâ”€â”€ .env # API keys
â””â”€â”€ README.md
```

## ğŸ” Environment Variables

Create a `.env` file:
```
GOOGLE_API_KEY=your_google_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=your_index_name
```
## ğŸ“¦ Installation

Clean install dependencies:

```bash
npm install langchain \
@langchain/community \
@langchain/google-genai \
@langchain/pinecone \
@pinecone-database/pinecone@5 \
pdf-parse@1
```

ğŸ§  Pinecone Index Setup

When creating your Pinecone index:

Dimension: 3072

Metric: cosine

Deployment: Serverless

âš ï¸ Dimension must match Gemini embedding output.

â–¶ï¸ Usage
1ï¸âƒ£ Index PDF
```
node indexing.js
```
This will:

Load PDF

Split into chunks

Generate embeddings (3072-dim)

Store vectors in Pinecone

2ï¸âƒ£ Ask Questions
```
node chatting.js
```

Then enter your question in the terminal.

ğŸ§® Embedding Model

```
gemini-embedding-001
```
Output dimension: 3072

Used for indexing + querying

âœ… Features

Semantic similarity search

Context-aware responses

Modular architecture

Clean separation of indexing and querying

Easily extendable to multi-document systems

ğŸ”® Future Improvements

Streaming responses

Source citations

API integration (Express.js)

Frontend UI

Multi-document ingestion

Reranking models

ğŸ“– What is RAG?

Retrieval-Augmented Generation improves LLM reliability by:

Retrieving relevant information

Injecting it as context

Generating grounded responses

This reduces hallucination and improves accuracy.

ğŸ‘¨â€ğŸ’» Author
Sagar Saini
Built with â¤ï¸ using Gemini + Pinecone + LangChain
